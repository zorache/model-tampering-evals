# Code for "Model Tampering Attacks Enables More Rigorous Evaluations of LLM Capabilities"

[workshop paper](https://openreview.net/pdf?id=XmvgWEjkhG) at NeurIPS Safe GenAI workshop
We release a suite of [unlearned models](https://huggingface.co/collections/LLM-GAT/) on Hugging Face across 8 checkpoints for the 10 unlearning methods we benchmarked.

